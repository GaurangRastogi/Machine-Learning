{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X= (hour sleeping , hour studying)  #y=test score of the student\n",
    "X=np.array(([2,9],[1,5],[3,6]),dtype=float)\n",
    "y=np.array(([92],[86],[89]),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Units\n",
    "X=X/np.amax(X,axis=0)  #amax -> gives max of every column(maximum of input array X)\n",
    "y=y/100 #maximum of test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "    #init is like constructor\n",
    "\n",
    "    def __init__(self):\n",
    "        #self is like this keyword\n",
    "\n",
    "        self.inputSize=2\n",
    "        self.outputSize=1\n",
    "        self.hiddenSize=3\n",
    "\n",
    "        #weights\n",
    "        self.W1=np.random.randn(self.inputSize,self.hiddenSize)         #(2x3) matrix for input to hidden layer #Sj x Sj+1\n",
    "        self.W2=np.random.randn(self.hiddenSize,self.outputSize)        #(3X1) matrix from hidden to output layer\n",
    "\n",
    "\n",
    "    #feedForward Neural Network \n",
    "    def feedForward(self,X):\n",
    "\n",
    "        # Forward Propogation to the network\n",
    "        self.z=np.dot(X,self.W1)    #dot product of X (input) and first set of Weights\n",
    "\n",
    "        #activation function (sigmoid function)\n",
    "        #second layer\n",
    "        self.z2=self.sigmoid(self.z)    #activation function\n",
    "\n",
    "        \n",
    "        self.z3=np.dot(self.z2,self.W2) #dot product of hidden layer (z2) and second set of weights (3x1)\n",
    "        #third layer\n",
    "        output=self.sigmoid(self.z3)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    #sigmoid function\n",
    "    def sigmoid(self,s,deriv=False):\n",
    "        if(deriv==True):        #When deriv parameter= True (which is False by default), it will calculate the derivative of sigmoid\n",
    "            return s* (1-s)\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "\n",
    "    #backWard propogation\n",
    "    def backwardPropogation(self,X,y,output):\n",
    "        #backward propogate through the network\n",
    "\n",
    "        #OutPUT Layer\n",
    "        self.output_error=y-output  #Error in output\n",
    "\n",
    "        #In backPropogation, to calculate delta ,we need to multipy output_error*sigmoid\n",
    "        self.output_delta=self.output_error*self.sigmoid(output,deriv=True) \n",
    "\n",
    "\n",
    "        #Hidden Layer\n",
    "        self.z2_error=self.output_delta.dot(self.W2.T)  #error in hidden layer #How much our hidden layer contributes to output error\n",
    "\n",
    "        #To calculate delta (or change we need to do in theta) for hidden layer \n",
    "        #Appyling derivative of sigmoid to z2_error\n",
    "        self.z2_delta=self.z2_error*self.sigmoid(self.z2,deriv=True)\n",
    "\n",
    "\n",
    "        #Update Weights\n",
    "        self.W1+=X.T.dot(self.z2_delta)     #adjusting first set (input -> hidden) weights\n",
    "        self.W2+=self.z2.T.dot(self.output_delta)   #adjusting second set of weights (hidden -> output) layer\n",
    "\n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        output=self.feedForward(X)\n",
    "        self.backwardPropogation(X,y,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3620005705485089\n",
      "Loss: 0.00025547846349232103\n",
      "Loss: 0.00023574267060605012\n",
      "Loss: 0.00021893988012185338\n",
      "Loss: 0.00020365771027873353\n",
      "Loss: 0.00018973968116170142\n",
      "Loss: 0.0001770461560017469\n",
      "Loss: 0.0001654523228157087\n",
      "Loss: 0.00015484673006779746\n",
      "Loss: 0.0001451299204959827\n",
      "Input: [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Final Loss: 0.0001362131729720149\n",
      "\n",
      "\n",
      "Predicted Output: [[0.90680808]\n",
      " [0.85711907]\n",
      " [0.9050437 ]]\n"
     ]
    }
   ],
   "source": [
    "#Creating Object \n",
    "NN=NeuralNetwork()\n",
    "\n",
    "for i in range(1000):   #train neural network 1000 times\n",
    "    if(i%100==0):\n",
    "        print(\"Loss: \" + str(np.mean(np.square(y-NN.feedForward(X)))))\n",
    "    NN.train(X,y)\n",
    "\n",
    "print(\"Input: \" + str(X))\n",
    "print(\"Actual Output: \"+ str(y))\n",
    "print(\"Final Loss: \" + str(np.mean(np.square(y-NN.feedForward(X)))))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted Output: \" + str(NN.feedForward(X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20643ba39cc192cecd775d36cbcb290681c3cfe28e501fd357f2d90d955dc5c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
